{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42f57e95-a897-4a57-9592-38915453de7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Carregar o Dataframe\n",
    "df_clients = spark.read.csv(\"dbfs:/FileStore/shared_uploads/matheus_rpg90@hotmail.com/default_os_credit_card_clients-2.csv\", header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "544faa7d-cafe-432a-b48c-37e3ca31b7b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Análise Exploratória dos Dados (EDA)\n",
    "### Fazendo processamento dos dados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3749b2da-5615-4fcf-b088-fa3f8e9625d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualizar o Dataframe\n",
    "display(df_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19a00038-92ba-4f5a-942b-52295fde077d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' O Dataframe está com os nomes das colunas na segunda linha vamos corrigir isso e colocá-los no cabeçalho para serem\n",
    "lidos corretamente como colunas'''\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Linha que contém os nomes reais das colunas\n",
    "header_row = df_clients.collect()[1] \n",
    "\n",
    "# Pega as 2 primeiras linhas\n",
    "rows_to_remove = df_clients.take(2)  \n",
    "\n",
    "# Remove linhas específicas do DataFrame.\n",
    "df_clean = df_clients.subtract(spark.createDataFrame(rows_to_remove, df_clients.schema))\n",
    "\n",
    "# Renomeia as colunas do DataFrame `df_clean`\n",
    "for i, col_name in enumerate(header_row):\n",
    "    df_clean = df_clean.withColumnRenamed(f\"_c{i}\", col_name)\n",
    "\n",
    "df_clients = df_clean\n",
    "\n",
    "# Mostra os nomes reais das colunas\n",
    "print(df_clients.columns)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d783188e-4c13-4d6f-b32b-b9fa815cf680",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Troca \"_\" por espaço e deixa tudo em minúsculo nos nomes das colunas\n",
    "df_clients = df_clients.toDF(*[col.lower().replace(\"_\", \" \") for col in df_clients.columns])\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d59d01ca-e57f-4cf7-a90f-0dbb7269e72f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cria um dicionário para renomear as colunas para melhorar o entendimento\n",
    "rename_columns = {\n",
    "    'id': 'client id',\n",
    "    'limit bal': 'credit limit',\n",
    "    'sex': 'gender',\n",
    "    'education': 'education level',\n",
    "    'marriage': 'marital status',\n",
    "    'age': 'age',\n",
    "    'pay 0': 'payment status sep',\n",
    "    'pay 2': 'payment status aug',\n",
    "    'pay 3': 'payment status jul',\n",
    "    'pay 4': 'payment status jun',\n",
    "    'pay 5': 'payment status may',\n",
    "    'pay 6': 'payment status apr',\n",
    "    'bill amt1': 'bill amount sep',\n",
    "    'bill amt2': 'bill amount aug',\n",
    "    'bill amt3': 'bill amount jul',\n",
    "    'bill amt4': 'bill amount jun',\n",
    "    'bill amt5': 'bill amount may',\n",
    "    'bill amt6': 'bill amount apr',\n",
    "    'pay amt1': 'payment amount sep',\n",
    "    'pay amt2': 'payment amount aug',\n",
    "    'pay amt3': 'payment amount jul',\n",
    "    'pay amt4': 'payment amount jun',\n",
    "    'pay amt5': 'payment amount may',\n",
    "    'pay amt6': 'payment amount apr',\n",
    "    'default payment next month': 'default next month'\n",
    "}\n",
    "# Renomeia as colunas com os dados do dicionário\n",
    "for old_name, new_name in rename_columns.items():\n",
    "    df_clients = df_clients.withColumnRenamed(old_name, new_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f87cdb3c-691d-4889-b34f-989fa45715b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualizando o dataframe\n",
    "display(df_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40118449-db10-4d2b-9aaf-2fe7482563c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verifica os tipos de dados de cada coluna\n",
    "df_clients.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a13803a-0176-40f3-8d8c-1a3a5e1c4917",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tranformando os tipos de dados para os tipos corretos\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "# Dicionário com os tipos desejados\n",
    "fixed_types = {\n",
    "    \"client id\": IntegerType(),\n",
    "    \"credit limit\": DoubleType(),\n",
    "    \"gender\": IntegerType(),\n",
    "    \"education level\": IntegerType(),\n",
    "    \"marital status\": IntegerType(),\n",
    "    \"age\": IntegerType(),\n",
    "    \"payment status sep\": IntegerType(),\n",
    "    \"payment status aug\": IntegerType(),\n",
    "    \"payment status jul\": IntegerType(),\n",
    "    \"payment status jun\": IntegerType(),\n",
    "    \"payment status may\": IntegerType(),\n",
    "    \"payment status apr\": IntegerType(),\n",
    "    \"bill amount sep\": DoubleType(),\n",
    "    \"bill amount aug\": DoubleType(),\n",
    "    \"bill amount jul\": DoubleType(),\n",
    "    \"bill amount jun\": DoubleType(),\n",
    "    \"bill amount may\": DoubleType(),\n",
    "    \"bill amount apr\": DoubleType(),\n",
    "    \"payment amount sep\": DoubleType(),\n",
    "    \"payment amount aug\": DoubleType(),\n",
    "    \"payment amount jul\": DoubleType(),\n",
    "    \"payment amount jun\": DoubleType(),\n",
    "    \"payment amount may\": DoubleType(),\n",
    "    \"payment amount apr\": DoubleType(),\n",
    "    \"default next month\": IntegerType()\n",
    "}\n",
    "\n",
    "# Aplicando as conversões de forma dinâmica\n",
    "for col_name, data_type in fixed_types.items():\n",
    "    df_clients = df_clients.withColumn(col_name, col(col_name).cast(data_type))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6d10ec2-3dce-49d1-bdad-32788e4f0a9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verifica os tipos de dados \n",
    "df_clients.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4770a1a8-9d02-4dc8-acc3-5ff7da9a1372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' As colunas education level\" e \"marital status\" possui dados inconsistentes é preciso corrigi-los'''\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Corrige os dados inconsistentes da coluna \"education level\" (valores inválidos: 0, 5, 6 → agrupar em 4 = \"outros\")\n",
    "df_clients = df_clients.withColumn(\"education level\", \n",
    "    when(col(\"education level\").isin(0, 5, 6), 4).otherwise(col(\"education level\")))\n",
    "\n",
    "# Corrige os dados inconsistentes da coluna \"marital status\" (valores inválidos: 0 → agrupar em 3 = \"outros\")\n",
    "df_clients = df_clients.withColumn(\"marital status\", \n",
    "    when(col(\"marital status\") == 0, 3).otherwise(col(\"marital status\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6b9e502-efd5-4a90-adcb-2c896ee71107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as _sum, isnan\n",
    "\n",
    "# Verifica a quantidade de dados faltantes\n",
    "df_clients.select([_sum(when(col(c).isNull() | isnan(c), 1).otherwise(0)).alias(c)\n",
    "               for c in df_clients.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f1b4a42-0f75-43da-911b-57770d31e518",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Conta as linhas duplicadas de todas as colunas\n",
    "duplicate_lines = df_clients.groupBy(df_clients.columns).count().filter(\"count > 1\")\n",
    "\n",
    "# Contar número total de duplicatas\n",
    "total_duplicates = duplicate_lines.selectExpr(\"sum(count - 1) as total\").collect()[0][\"total\"]\n",
    "print(f\"Total de linhas duplicadas: {total_duplicates}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35d51a37-3f02-4f26-9ea8-24d5b78f73d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Análise Estatística dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "249eca45-bada-488a-aca0-4c14a8ed8e1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verifica o balanceamento dos dados da varável alvo \"default next month\"\n",
    "df_clients.groupBy(\"default next month\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b82d587-d5af-4a28-9ba6-bcd821130094",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Faz análise descritiva das variáveis\n",
    "df_clients.describe().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7afbbd5d-aff2-4a79-89d3-6f35cd6a481d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' Verificando a correlação entre a variável alvo e as variáveis, em problemas desse tipo como classificação\n",
    "é esperado que as variáveis não possuam correlação forte mas gosto sempre de verificar'''\n",
    "\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "numeric_cols = [c for c, dtype in df_clients.dtypes if dtype in (\"double\", \"int\") and c != \"default next month\"]\n",
    "\n",
    "for col_name in numeric_cols:\n",
    "    corr = df_clients.stat.corr(col_name, \"default next month\")\n",
    "    print(f\"Correlação entre {col_name} e default next month: {corr:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88e6df23-8cd2-4ab2-9579-a4177c977a06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# verifica a frequência de variáveis categóricas por classe\n",
    "\n",
    "categorical_cols = [\"gender\", \"education level\", \"marital status\"]\n",
    "\n",
    "for col_name in categorical_cols:\n",
    "    print(f\"Frequência de {col_name} por classe 'default next month':\")\n",
    "    df_clients.groupBy(\"default next month\", col_name).count().orderBy(col_name, \"default next month\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4e26241-b475-44d4-a085-357540994fdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Treinamento dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d0f4240-a828-4d92-a3ec-7b696c0237ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# 1. Montar vetores de features\n",
    "feature_cols = [col for col in df_clients.columns if col != \"default next month\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df_vector = assembler.transform(df_clients).select(\"features\", \"default next month\")\n",
    "\n",
    "# 2. Dividir treino e teste (estratificação manual não disponível, mas pode comparar classes após split)\n",
    "train, test = df_vector.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 3. RANDOM FOREST\n",
    "rf = RandomForestClassifier(labelCol=\"default next month\", featuresCol=\"features\", seed=42)\n",
    "rf_model = rf.fit(train)\n",
    "rf_preds = rf_model.transform(test)\n",
    "\n",
    "# 4. AVALIAÇÃO - Random Forest\n",
    "y_true_rf = np.array(rf_preds.select(\"default next month\").collect()).flatten()\n",
    "y_pred_rf = np.array(rf_preds.select(\"prediction\").collect()).flatten()\n",
    "y_score_rf = np.array(rf_preds.select(\"probability\").rdd.map(lambda x: float(x[0][1])).collect())\n",
    "thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred_custom = (y_score_rf >= t).astype(int)\n",
    "    f1 = f1_score(y_true_rf, y_pred_custom)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = t\n",
    "\n",
    "print(f\"Melhor Threshold com base no F1-score: {best_threshold:.2f}\")\n",
    "print(f\"Novo F1-score: {best_f1:.4f}\")\n",
    "\n",
    "\n",
    "y_pred_custom = (y_score_rf >= best_threshold).astype(int)\n",
    "\n",
    "print(\"\\n Métricas com threshold ajustado\")\n",
    "print(\"F1-score:\", round(f1_score(y_true_rf, y_pred_custom), 4))\n",
    "print(\"Precision:\", round(precision_score(y_true_rf, y_pred_custom), 4))\n",
    "print(\"Recall:\", round(recall_score(y_true_rf, y_pred_custom), 4))\n",
    "print(\"ROC-AUC (não muda):\", round(roc_auc_score(y_true_rf, y_score_rf), 4))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "522664a2-9546-43e7-b9eb-4bba069a592a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "from pyspark.sql.functions import when\n",
    "import numpy as np\n",
    "\n",
    "# 1. Montar vetores de features\n",
    "feature_cols = [col for col in df_clients.columns if col != \"default next month\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df_vector = assembler.transform(df_clients).select(\"features\", \"default next month\")\n",
    "\n",
    "# 2. Calcular pesos para classWeightCol\n",
    "class_counts = df_vector.groupBy(\"default next month\").count().toPandas()\n",
    "count_0 = class_counts[class_counts[\"default next month\"] == 0][\"count\"].values[0]\n",
    "count_1 = class_counts[class_counts[\"default next month\"] == 1][\"count\"].values[0]\n",
    "total = count_0 + count_1\n",
    "weight_0 = total / (2 * count_0)\n",
    "weight_1 = total / (2 * count_1)\n",
    "\n",
    "# 3. Criar coluna de pesos\n",
    "df_weighted = df_vector.withColumn(\n",
    "    \"weight\",\n",
    "    when(df_vector[\"default next month\"] == 0, weight_0).otherwise(weight_1)\n",
    ")\n",
    "\n",
    "# 4. Dividir treino e teste\n",
    "train, test = df_weighted.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 5. Treinar GBT com pesos\n",
    "gbt = GBTClassifier(\n",
    "    labelCol=\"default next month\",\n",
    "    featuresCol=\"features\",\n",
    "    weightCol=\"weight\",\n",
    "    seed=42,\n",
    "    maxIter=100\n",
    ")\n",
    "gbt_model = gbt.fit(train)\n",
    "gbt_preds = gbt_model.transform(test)\n",
    "\n",
    "# 6. Avaliação com threshold padrão\n",
    "y_true_gbt = np.array(gbt_preds.select(\"default next month\").collect()).flatten()\n",
    "y_score_gbt = np.array(gbt_preds.select(\"probability\").rdd.map(lambda x: float(x[0][1])).collect())\n",
    "y_pred_gbt = np.array(gbt_preds.select(\"prediction\").collect()).flatten()\n",
    "\n",
    "print(\"Métricas - GBT (threshold padrão = 0.5)\")\n",
    "print(\"F1-score:\", round(f1_score(y_true_gbt, y_pred_gbt), 4))\n",
    "print(\"Precision:\", round(precision_score(y_true_gbt, y_pred_gbt), 4))\n",
    "print(\"Recall:\", round(recall_score(y_true_gbt, y_pred_gbt), 4))\n",
    "print(\"ROC-AUC:\", round(roc_auc_score(y_true_gbt, y_score_gbt), 4))\n",
    "\n",
    "# 7. Ajustar threshold com base no melhor F1-score\n",
    "thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "best_f1 = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred_thresh = (y_score_gbt >= t).astype(int)\n",
    "    f1 = f1_score(y_true_gbt, y_pred_thresh)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = t\n",
    "\n",
    "y_pred_adjusted = (y_score_gbt >= best_threshold).astype(int)\n",
    "\n",
    "# 8. Avaliação com threshold ajustado\n",
    "print(\"\\n Métricas - GBT com classWeightCol + threshold ajustado\")\n",
    "print(\"Melhor threshold:\", round(best_threshold, 2))\n",
    "print(\"F1-score:\", round(f1_score(y_true_gbt, y_pred_adjusted), 4))\n",
    "print(\"Precision:\", round(precision_score(y_true_gbt, y_pred_adjusted), 4))\n",
    "print(\"Recall:\", round(recall_score(y_true_gbt, y_pred_adjusted), 4))\n",
    "print(\"ROC-AUC (não muda):\", round(roc_auc_score(y_true_gbt, y_score_gbt), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "330d7041-2617-4da3-ac33-658e6bc98515",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import mlflow\n",
    "    print(\"MLflow está instalado!\")\n",
    "except ImportError:\n",
    "    print(\"MLflow NÃO está instalado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eba0a75e-48e9-4e9c-92a0-03e9058181e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Salvando o melhor modelo e simulando deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a682f10e-2213-4a96-829c-068552883eab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip uninstall -y typing_extensions\n",
    "%pip install typing_extensions==4.5.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36300fab-e86a-4d4b-b62e-fde51b9dead5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "150ac2d8-d2dc-4913-97f6-5c3c413253cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "# Define experimento local (ou dentro de uma pasta de usuário)\n",
    "mlflow.set_experiment(\"/Users/matheus_rpg90@hotmail.com/experiment_gbt\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"GBTClassifier_model\"):\n",
    "\n",
    "    # Log de parâmetros e métricas\n",
    "    mlflow.log_param(\"maxIter\", 100)\n",
    "    mlflow.log_param(\"threshold_otimizado\", round(best_threshold, 2))\n",
    "    mlflow.log_metric(\"f1_score\", round(f1_score(y_true_gbt, y_pred_adjusted), 4))\n",
    "    mlflow.log_metric(\"precision\", round(precision_score(y_true_gbt, y_pred_adjusted), 4))\n",
    "    mlflow.log_metric(\"recall\", round(recall_score(y_true_gbt, y_pred_adjusted), 4))\n",
    "    mlflow.log_metric(\"roc_auc\", round(roc_auc_score(y_true_gbt, y_score_gbt), 4))\n",
    "\n",
    "    # Salva o modelo treinado no MLflow\n",
    "    mlflow.spark.log_model(gbt_model, \"model\")\n",
    "\n",
    "    print(\"Modelo salvo e logado no MLflow com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e35f28d-66a7-47f8-ac1f-7d7ec242ab53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo: carregar o último modelo logado no MLflow\n",
    "from mlflow.spark import load_model\n",
    "\n",
    "# Caminho do modelo salvo (ajuste para o nome correto do run_id ou caminho do modelo)\n",
    "model_uri = \"runs:/8b8a561de9604eef83abf585e9911af9/model\"  # Você verá esse ID na UI do MLflow\n",
    "\n",
    "# Carrega o modelo\n",
    "loaded_model = load_model(model_uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ced3a07e-f128-49f1-93c7-662a0d8d76ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Montar novos dados (com as mesmas colunas de treino)\n",
    "novos_dados = df_clients.limit(3)\n",
    "novos_vetor = assembler.transform(novos_dados).select(\"features\")\n",
    "\n",
    "# Fazer previsão\n",
    "previsao = loaded_model.transform(novos_vetor)\n",
    "previsao.select(\"prediction\", \"probability\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fe1441c-b532-48f1-a7d0-106f73c218dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "novo_cliente = spark.createDataFrame([{\n",
    "    \"client id\": 20001,\n",
    "    \"credit limit\": 20000.0,\n",
    "    \"gender\": 1,\n",
    "    \"education level\": 2,\n",
    "    \"marital status\": 1,\n",
    "    \"age\": 34,\n",
    "    \"payment status sep\": -1,\n",
    "    \"payment status aug\": -1,\n",
    "    \"payment status jul\": 0,\n",
    "    \"payment status jun\": 0,\n",
    "    \"payment status may\": 0,\n",
    "    \"payment status apr\": 0,\n",
    "    \"bill amount sep\": 5000.0,\n",
    "    \"bill amount aug\": 4500.0,\n",
    "    \"bill amount jul\": 4000.0,\n",
    "    \"bill amount jun\": 3500.0,\n",
    "    \"bill amount may\": 3000.0,\n",
    "    \"bill amount apr\": 2500.0,\n",
    "    \"payment amount sep\": 1000.0,\n",
    "    \"payment amount aug\": 1000.0,\n",
    "    \"payment amount jul\": 1000.0,\n",
    "    \"payment amount jun\": 1000.0,\n",
    "    \"payment amount may\": 1000.0,\n",
    "    \"payment amount apr\": 1000.0\n",
    "}])\n",
    "\n",
    "# ---------------------\n",
    "# 3. Monta o vetor de features\n",
    "# ---------------------\n",
    "feature_cols = [col for col in novo_cliente.columns if col != \"default next month\"]  # Ignora alvo, se presente\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "novo_cliente_vector = assembler.transform(novo_cliente).select(\"features\")\n",
    "\n",
    "# ---------------------\n",
    "# 4. Faz a previsão\n",
    "# ---------------------\n",
    "previsao = loaded_model.transform(novo_cliente_vector)\n",
    "previsao.select(\"prediction\", \"probability\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook sem título 2025-04-28 15:34:13",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}